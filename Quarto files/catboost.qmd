---
title: Catboost Model
jupyter: python3
---


In this notebook, a final sales forecasting model based on the CatBoost algorithm is built. More information can be found here: https://catboost.ai/

Just as the Random Forest model, the final model is also a tree-based model. These types of algorithms lend themselves well to predicting target features that are not normally distributed, as is the case with sales here. Moreover, CatBoost is specifically tailored to datasets with many categorical variables. With the expception of weather and lag variables, this dataset mainly contains categorical features, including dummy variables. Being a tree-based model, Catboost can handle non-linear patterns very well, which are likely to occur in sales data.

Given the flexibility of CatBoost, no additional preprocessing is necessary. It can handle object (categorical) variables, missing values and responds well to a non-normally distributed target variable and non-linear relationships between the predictors and the target variable.


```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as md
from matplotlib.patches import Patch
import seaborn as sns
import datetime as dt

from sklearn.preprocessing import OneHotEncoder, RobustScaler, FunctionTransformer, PolynomialFeatures, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error
import sktime

from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold
from statsmodels.tsa.stattools import adfuller

from catboost import CatBoostRegressor
import catboost as cb
import optuna
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
#sktime libraries

from sktime.split import ExpandingWindowSplitter
from sktime.utils import plot_windows, plot_series
from sktime.transformations.series.summarize import WindowSummarizer
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
#| collapsed: true
# for google colab
#!unrar x drive/MyDrive/sweet-spot-donut-sales-forecasting.rar
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
! jupyter nbextension enable widgetsnbextension --py
```

```{python}
%run Function_scripts/functions_model.py
%run Function_scripts/functions_vis.py
```

```{python}
pd.set_option("display.max_columns", None)
```

### Loading the dataset

```{python}
d = pd.read_csv("data/train_df.csv")
d_test = pd.read_csv("data/test_df.csv")
```

```{python}
d['date'] = pd.to_datetime(d['date'])
d_test['date'] = pd.to_datetime(d_test['date'])
```

## Modelling

#### Selecting features

In order for the CatBoost algorithm to recognize categorical features, the variables were first saved either as categorical or numerical. The date variable will serve as the index. The remaining window features (e.g. lag variables or rolling averages) will be included at a later step, but only in the training dataset. Adding window variables to the test dataset will be done recursively during the prediction phase. More information on how this is done can be found below.

```{python}
date = ["date"]

catfeat = ["store_name","item_category",'box_deal', 'day', 'halloween', 'hol_pub', 'hol_school',
       'month', 'nye', 'public_space', 'street_market', 'valentines_day','week_year', 'weekday', 'year']

numfeat = ["days_back","temperature_2m_mean","sunshine_duration","precipitation_hours"]
```

### Evaluation metric

There are several metrics that can be used to evaluate the perfomance of a model. 

* R-Squared (R^2^)
* Root Mean Squared Error (RMSE)
* Mean Absolute Error (MAE)
* Mean Absolute Percentage Error (MAPE)

The RMSE will be used as a loss metric for model training. The model itself will be evaluated based on **R^2^ and MAPE** scores.

*R^2^* represents the proportion of the variance in the dependent variable that is predictable from the independent (explanatory) variables. For example, an R-squared of 0.8 means that 80% of the variance in the target variable is explained by the model. It is a basic metric that is useful when comparing the accuracy of different models.

*MAPE*, on the other hand, is an error metric. Here are some benefits of using *MAPE*:

* MAPE is expressed as a percentage, making it easy to interpret and understand. A MAPE of 5% means that, on average, the forecast is off by 5% from the actual value, which is intuitively meaningful for most stakeholders. 
* MAPE also penalizes both overestimations and underestimations equally, which is important in the context of forecasting donut sales and curbing food waste. On the one hand, overestimations are harmful insofar as they may cause overproduction, leading to more resources being used than necessary. On the other hand, underestimations lead to underproduction and the store not being able to meet customer demand.
* Finally, MAPE makes it easy to compare the accuracy of models across different datasets with varying scales and units.



### Hyperparameter Tuning with Cross-validation

Training tree-based algorithm often leads to overfitting. In order to create a generalizable model, it is necessary to "prune" the trees by putting constraints on how well they can learn from the data. The following hyperparameters are tuned in the step below:

* Number of trees
* Learning rate
* L2 leaf regularization
* Tree depth (number of levels)
* Proportion of the sample used for each tree
* Proportion of the features used for each tree
* A minimum amount of datapoints in a leaf before a split can occur
* Minimum child weight

```{python}
d2 = d[date + catfeat + numfeat + ["total_amount"]]
d_test2 = d_test[date + catfeat + numfeat + ["total_amount"]]
```

```{python}
agg_columns = d2.columns.difference(['date', 'store_name', 'item_category'] + ["total_amount"])
agg_dict = {col: "first" for col in agg_columns}
agg_dict["total_amount"] = "sum"

d2 = d2.groupby(['date', 'store_name', 'item_category']).agg(agg_dict).reset_index().sort_values(by = "date", ascending = False).reset_index(drop = True)
d2["hol_pub"] = d2["hol_pub"].apply(np.int64)
d2["hol_school"] = d2["hol_school"].apply(np.int64)

d2 = d2.set_index(["store_name","item_category","date"]).sort_index()

d_test2 = d_test2.groupby(['date', 'store_name', 'item_category']).agg(agg_dict).reset_index().sort_values(by = "date", ascending = False).reset_index(drop = True)
d_test2["hol_pub"] = d_test2["hol_pub"].apply(np.int64)
d_test2["hol_school"] = d_test2["hol_school"].apply(np.int64)

d_test2 = d_test2.set_index(["store_name","item_category","date"]).sort_index()
```


#### Creating window variables

**Window variables** are essential in modelling sales forecasts. They tend to capture the effect of past sales on current sales very well. Window variables include sales from the previous days (so called *lagged variables*) as well as *moving averages* and *moving standard deviation* of sales over a specified past time period. 

After other exogenous varaibles, such as holidays or weather, are taken into account as predictors of sales, what usually remains are residuals that sometimes exert a pattern. Because this pattern was not captured by exogenous variables, windowed variables are often able to systematically explain a lot of the remaining noise extremely well. Including window variables as predictors should therefore significantly improve the model's predictions.

The following window variables are included as additional predictive features.
* Lagged variables (sales from 1, 2 and 3 days ago) 
* Moving averages (average sales over the past 7, 15 and 30 days)
* Moving standard deviation (standard deviation over the past 4 days)

```{python}
kwargs = {"lag_feature": {
    "lag":[1,2,3],
    "mean": [[1,7], [1, 15], [1,30]],
    "std": [[1,4]]
    },
    "target_cols":["total_amount"]}

transformer = WindowSummarizer(**kwargs, n_jobs= -1)
```

```{python}
d2wind = transformer.fit_transform(d2)
```

```{python}
d2wind = pd.concat([d2["total_amount"], d2wind], axis = 1).dropna()
```


### Creating validation dataset
A validation set, containing the last 7 days of the entire training dataset, is created. The validation set is used for evaluation purposes and early stopping in the CatBoost model.

```{python}
train, val = create_val_set(d2wind.reset_index())
train = train.set_index(["store_name","item_category","date"]).sort_index()
train = train.iloc[train.index.get_level_values("date") >= pd.to_datetime("2021-07-12")]
val = val.set_index(["store_name","item_category","date"]).sort_index()

for col in catfeat[2:]:
    train[col] = train[col].apply(np.int64)
    val[col] = val[col].apply(np.int64)
```

**Time-series cross-validation split**

When performing k-fold cross-validation in a time-series context, the data musn't be shuffled as in a regular cross-validation scenario. Instead, a forecasting horizon has to be defined for each fold, which serves as the validation set. It always comes after the training set chronologically. Because predicting donut sales for more than one week in advance is a difficult endeavor and will most likely lead to imprecise results, the forecasting horizon for this problem was set to 7 days.

The graph below depicts the time-series cross-validation process visually. The forecsasting horizon of 7 days from the first fold becomes part of the training set in the second cross-validation fold, while the next 7 days serve as the second validation set. Five folds were chosen in total, meaning there are 35 validation days on which the training data is evaluated.

</br>

**Note:**

---

When making time-series forecasting on a test dataset, predictions are often done recursively. This means that the model does not make predictions for all points in a forecasting horizon at once (7 days in this case), but does so day by day. The reason for that is that when window variables - such as lagged features or rolling averages of the target variable (donut sales) - are taken into account, they don't occur in the test dataset as explanatory variables. They are, in that case, calculated based on future target variable values which are yet to be predicted.

This type of assessment will be performed on the testing dataset at the end of the notebook . The cross-validation process in this case would not only involve recursive predictions, but would also have to accommodate hierarchical data structures (i.e. one store per one product per one day).

In order to simplify this task, no recursive predictions will be carried out on either of the five validation folds at this point. While this won't yield the most optimal predictions, performing time-series cross-validation can still produce good evaluation results.

To make the results more rigorous, recursive forecasting in cross-validation should be implemented in the future.

```{python}
cv_list = window_splitter_prep(train, validation_length = 7, num_folds = 5)
```

```{python}
for i in range(32,33):
    plot_windows(list(cv_list[i][0].values())[0], list(cv_list[i][1].values())[0]["total_amount"])
```

**Hyperparameter tuning**

Given that CatBoost is a tree-based model, there is a large number of hyperparameters that can be tuned. For this task, the following hyperparameters were chosen:

* Number of trees (estimators)
* Learning rate
* L2 leaf regularization
* Tree depth
* Percentage of datapoint to be used in each tree (bootstrapping)
* Percentage of features to be used in each tree
* Smallest number of data to be used in a leaf before a split is made

```{python}
x_train = train.reset_index().drop("total_amount", axis = 1)
x_val = val.reset_index().drop("total_amount", axis = 1).set_index("date")
y_train = train.reset_index()[["date","total_amount"]]
y_val = val.reset_index()[["date","total_amount"]].set_index("date")
```

```{python}
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 200, 3000),
        "learning_rate": trial.suggest_float("learning_rate", 1e-4, 0.1, log=True),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 2, 30),
        "depth": trial.suggest_int("depth", 5, 10),
        "subsample": trial.suggest_float("subsample", 0.05, 1.0),
        #"colsample_bylevel": trial.suggest_float("colsample_bylevel", 0.1, 1.0),
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 10, 100),
        "random_strength": trial.suggest_float("random_strength", 1, 10)
    }

    scores = []

    for train_idx, test_idx in create_train_validation_folds(x_train):
        X_train_fold, X_val_fold = x_train.iloc[train_idx], x_train.iloc[test_idx]
        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[test_idx]

        X_train_fold = X_train_fold.set_index("date")
        X_val_fold = X_val_fold.set_index("date")
        y_train_fold = y_train_fold.set_index("date")
        y_val_fold = y_val_fold.set_index("date")

        model = CatBoostRegressor(**params, loss_function = "RMSE", silent=True,
                                  allow_writing_files = False, task_type="GPU", bootstrap_type="Poisson")

        model.fit(X_train_fold, y_train_fold, eval_set = (x_val, y_val), early_stopping_rounds = 50, cat_features=catfeat)

        y_pred = model.predict(X_val_fold)

        score = mean_squared_error(y_val_fold, y_pred, squared=False)

        scores.append(score)

    return np.mean(scores)

```

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials = 15)
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
print('Best hyperparameters:', study.best_params)
print('Best RMSE:', study.best_value)
```

```{python}
best_parameters = study.best_params
del best_parameters["n_estimators"]
```

```{python}
x_train = x_train.set_index("date")
y_train = y_train.set_index("date")
```

```{python}
best_parameters = {'learning_rate': 0.0948724560273695,
 'l2_leaf_reg': 20.175788478590995,
 'depth': 9,
 'subsample': 0.3228633492949712,
 'min_data_in_leaf': 93,
 'random_strength': 4.688536316741192}

model = CatBoostRegressor(**best_parameters, n_estimators = 3000, colsample_bylevel = 0.2, allow_writing_files = True, random_state=132)

model.fit(x_train, y_train, eval_set=(x_val, y_val),
          early_stopping_rounds = 200, 
          cat_features = catfeat, verbose = 250, plot = True)
```

```{python}
best_parameters2 = {'learning_rate': 0.0302289424483556, 
                    'l2_leaf_reg': 17.454469359806595, 
                    'depth': 10, 
                    'subsample': 0.6081282597582008, 
                    'min_data_in_leaf': 35,
                    'random_strength': 4.799266038182125}

model2 = CatBoostRegressor(**best_parameters2, n_estimators = 3000, colsample_bylevel = 0.2, allow_writing_files = True, random_state=132)

model2.fit(x_train, y_train, eval_set=(x_val, y_val),
          early_stopping_rounds = 200, 
          cat_features = catfeat, verbose = 250, plot = True)
```


**Saving the model**

```{python}
model2.save_model("saved_models/cb_final_model.cbm")
```


### Feature importances

```{python}
model2 = CatBoostRegressor()

model2.load_model("saved_models/cb_final_model.cbm")
```

```{python}
feat_importances = pd.DataFrame({
    'Feature': x_train.columns,
    'Importance': model2.get_feature_importance()
}).sort_values("Importance", ascending = False).head(20)
```

```{python}
#| colab: {base_uri: 'https://localhost:8080/', height: 652}
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feat_importances, palette='viridis')

plt.title('Feature Importances from Catboost Regressor')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()
```

The most important features that explain sales are mainly related to temporal variables. Most notably, the model confirms the original assumption different weekdays - especially weekend days - are good predictors of sales.

Window variables, especially moving averages, also explain sales trends well. Interestingly, it is those window variables that are not captured by the overall seasonal pattern variables (e.g. months or years) that play major roles in accurately forecasting sales. These are specifically moving averages of the sales variable over the past 15 days as well as sales from the day before (lag 1 variable).

Some important exogenous features are, expectedly, the item categories and the individual stores. The "store" feature encapsulates not only the store itself, but also the different locations with all their specificities in terms of sociodemographics, urban environments and people's purchasing power. Customers will naturally also prefer varying products, which was seen in the visualisation notebook. It was therefore important for the model to capture sales amongst different donut types. The high importance of this variable attests to this.

Although not as pronouced, other exogenous variables, such as New Year's Eve and weather, also played a relevant role in forecasting sales. 


### Evaluation metrics train and validation set

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
y_train_pred = model.predict(x_train)
print(f"R-squared train: {round(r2_score(y_train, y_train_pred),6)}")
print(f"MAPE train: {round(100*mean_absolute_percentage_error(y_train, y_train_pred),2)}\n")

y_val_pred = model.predict(x_val)
print(f"R-squared validation: {round(r2_score(y_val, y_val_pred),6)}")
print(f"MAPE validation: {round(100*mean_absolute_percentage_error(y_val, y_val_pred),2)}")
```

```{python}
y_train_pred2 = model2.predict(x_train)
print(f"R-squared train: {round(r2_score(y_train, y_train_pred2),6)}")
print(f"MAPE train: {round(100*mean_absolute_percentage_error(y_train, y_train_pred2),2)}\n")

y_val_pred2 = model2.predict(x_val)
print(f"R-squared validation: {round(r2_score(y_val, y_val_pred2),6)}")
print(f"MAPE validation: {round(100*mean_absolute_percentage_error(y_val, y_val_pred2),2)}")
```


## Test dataset

```{python}
train_final = d2wind.iloc[d2wind.index.get_level_values("date") >= pd.to_datetime("2021-07-12")].reset_index().set_index("date")

x_test_final = d_test2.reset_index().set_index("date").drop("total_amount", axis = 1)
y_test_final = d_test2.reset_index()[["date","total_amount"]].set_index("date")
```

The prediction of sales was done recursively, meaning that every day's sales forecasts in the test dataset were performed consecutively. Put differently, sales, lagged and window variables (e.g. rolling averages) were predicted only for the next day. Predictions were thereby used iteratively as inputs for window variables, which rely on the target variable's predicted values. 

The final result of this iterative process are more accurate sales predictions in the test dataset.

```{python}
x_test_final, y_test_pred = pred_test(train = train_final, test = x_test_final, model = model2)
```

### Evaluation metrics train and test set

The final evaluation metrics show the R^2^ and Mean Absolute Percentage Error (MAPE) both for the training and the test dataset. In addition, the MAPE is displayed for each store and at each product category level.

With a **test MAPE of 17% and R^2^ of 0.96**, the model did a very good job at predicting sales for the next seven days.

```{python}
fit_overview(ytrain = y_train, ytrainpred = y_train_pred2, ytest = y_test_final, ytestpred = y_test_pred)
```

Breaking the scores further down by stores and product categories, it can be seen the MAPE is even lower for some stores that have been open longer. This is true at both the store level and the individual store-product pairs.

```{python}
# breakdown: either "stores" or "stores_products"

mape_stores(d_test2, y_test_pred, breakdown = "stores").sort_values("MAPE").reset_index(drop = True)
```

```{python}
# breakdown: either "stores" or "stores_products"

mape_stores(d_test2, y_test_pred, breakdown = "stores_products").sort_values("MAPE").reset_index(drop = True)
```


### Residual Plot Analysis

The residual plot below shows the difference between the predicted and actual sales. Residuals were standardized with a mean 0.

```{python}
difference_df = diff_overview(data = d_test2, pred = y_test_pred)
```

```{python}
residual_plot(difference_df)
```

The first plot shows residuals excluding total daily sales. The second plot shows residuals with reference only to daily sales. The following observations can be made:

* In terms of the distribution of residuals, both plots look similar to those of the Random Forest model. The conclusions drawn from there therefore apply to the Catboost model as well.

* The residuals for the different product categories seem normally distributed, with most of them being centered around the mean and within 1 standard deviation. 

* The product category plot seems to be rather heteroskedastic. Most notably, the mixed category - which sells boxes of different types of donuts - and monthly specials have relatively high residual errors, at least compared to other categories. Considering that most sold donuts belonged to the *mixed* category, it is no surprise that the model had difficulties predicting relatively high values correctly. The error distribution reveals that there may be some patterns in the dataset that the model didn't capture well enough and that could explain the sales trend. Another possibility is that some other variables - for example advertising or other special events - serve as better predictors for sales for these two categories. Alternatively, reclassfying the *mixed* group may yield a more precise represenatation of distinct donut categories that could individually explain sales better than the larger category.

* In the total daily sales plot, there are only a few points with low daily sales. These sales are from stores that opened recently. Apart from that, the plot looks more or less homoskedastic, meaning there are no clear patterns.

### Visualisation of predictions

```{python}
df_predicted = pd.concat(
    [
    d2.reset_index()[(d2.reset_index()["date"] >= pd.to_datetime("2024-05-01"))][["date","store_name","item_category","total_amount"]],
    difference_df[["Date","Store name","Product category","Observed","Predicted"]].rename(columns = {"Date":"date","Store name":"store_name",
                                                                                                     "Product category":"item_category","Observed":"total_amount"})
    ]
    ).sort_values(["date","store_name","item_category"]).reset_index(drop = True)
```

```{python}
# specify item to view by product category: classics, mixed, monthly_specials, specials or daily total

ts_predicted(df_predicted, item = "specials")
```

```{python}
ts_predicted(df_predicted, item = "daily total")
```

The visualisation of the results in the form of time-series line plots confirms the accuracy of the model. Looking at the dates between May 1 and May 31 2024, high sales fluctuations are observable across all stores, whether it's daily total sales or the sales of, for instance, donuts from the "specials" category (graphs above). 

Despite these fluctuations, some patters are still recognizable, both after the cutoff date for the test set, as well as after. While not perfect for all stores, the model still captures the daily sales trends and forecasts very well all across the board. This is especially true for the first 2-3 days that the model is predicting for. Compared to the Random Forest model, the Catboost model does a better job capturing patterns and makes more accurate forecasts.

All in all, with a **MAPE of ~15%**, the model is able to **forecast donut sales of more than a half store-product combinations quite well**. These include products such as classic donuts, mixed boxes and monthly specials as well as total daily sales. Ignoring the 3 stores that opened up most recently and that lack quality data, the model's three worst MAPE scores for remaining store-product pairs are 27%, 26% and 19%, respectively. While these predictions can and should be improved, they are certainly better than the Random Forest's worst MAPE scores (35%, 34% and 32%).

**The clear winner is therefore the Catboost model, which boasts better R2 and MAPE scores, both at the aggregated level and the individual store-product levels.**

</br>


